
### Preliminaries / Chapter 1
- Why study programming languages?
    - improve understanding of languages you know
    - improve choice of languages for applications
    - improve ability to learn new languages
    - choose among alternative ways to express idea
    - make it easier to design new languages
- What makes a language successful?
    + Ease of use for the novice
    + Expressive power
    + Ease of implementation, wide dissemination at minimal cost
    + Excellent compilers, possible to compile to very good (fast/small) code
    + Backing of a powerful sponsor
- Programming Domains
    + Scientific applications 
        - large numbers of floating point computations; use of arrays
        - e.g. FORTRAN
    + Business Applications
        * Produce reports, use decimal number and characters
        * e.g. COBOL 
    + Artificial Intelligence 
        * Manipulate symbols rather than numbers; use of linked lists 
        * e.g. LISP
    + Web Software
        * Eclectic collection of languages:
            - Markup (e.g. XHTML)
            - General Purpose (e.g. Java, Kotlin, Ruby)
            - Scripting (e.g. Javascript, PHP)
                + Scripting language - A scripting or script language is a programming language for a special run-time environment that automates the execution of tasks (e.g. python - "script")
- Domain-specific Language (DSL)
    + *a language specifically designed to perform tasks in a certain domain*
    + Non-DSL examples: C, C++, COBOL, Fortan, Java
        + They are not small
        + b.c. of their expressive power, they are not restricted to the domains each was linked to the previous slides 
    + DSL examples: CSS, SQL(Structured Query Language), UML(Unified Modeling Language)
        * In many cases a programming can be done by a *domain expert* who is not necessarily a seasoned programmer

- Language Evaluation Criteria
    + Writability
        * Simplicity and ***orthogonality***
            - a relatively small set of *primitive* constructs
            - a *consistent* set of rules for combining them
            - every possible combination is *legal* and *meaningful*
            - (orthogonal comes from math - "independence")
                + e.g. a single instruction (+) can use either registers or memory cells as the operands.
        - Support for **abstraction**
            + The ability to define and use complex structures of operations in ways that allow details to be ignored
        + Expressivity
            * a set of relatively convenient ways pf specifying operations
            * strength and number of operators and predefined functions
    + Readability
        * Simplicity and orthogonality
            - similar to writability
        - More on simplicity
            - Minimal feature multiplicity 
                + `x = x + 1;` `x += 1` `x++` `++x` 
            - Minimal operator overloading
                + e.g. can't overload operator in c++
        + Data Types and structures
            * Adequate predefined data types and structures
            * e.g. C do not have a boolean type originally
            * e.g. float and double
        * Syntax considerations
            - self-descriptive constructs, meaningful keywords
            - methods of forming compound statements `if {...}` vs. `if ...endif`
    + Reliability
        * Type checking
            - Testing for type errors, whether compile-time or run-time
            - preventing a bit-pattern representing one type of data from being interpreted as a different type of data 
        - Exception handling
            + Intercept run-time errors and take corrective measures
        + *Aliasing* (reduce reliability)
            * Presence of two or more distinct referencing methods for the same memory location
        * Readability and writability
            - a language that does not support "natural" ways of expressing an algorithm will require the use of "unnatural" approaches
    + Cost 
        * Considerations
            - Writing programs - Closeness in purpose to the application
            - Reliability - Poor reliability leas to high costs
            - Maintaining programs
        - Maintaining Duplicated Code (Cloned Code)
            + what if there's bug in the code clones
            + Hard for someone else to maintain
    + Others
        * Portability / the ease with which programs can be moved from one implementation to another
            - java compile once run an any java JVM - java virtual machine
        - Generality / applicability to a wide range of applications
        - Well-definedness / the completeness and precision of the language's official definition document
            + (design compiler based on the specification)

- Two Main influences on language design
    + **computer architecture**
        * **the von neumann architecture** 
            - the stored-program computer
            - fetch-decode-execute
            - *imperative languages* are most dominant b.c. of von Neumann computers
            - Memory is *separate* from CPU
            - *Data* and *programs* both stored in memory
            - instructions/data are *piped* from memory to CPU
            - Basis for imperative languages
                + *Variables* model memory cells
                + *Assignment statements* model piping
                + Efficient iterative form of repetition
    + **Programming Methodologies**
        * New software development methodologies (e.g. object-oriented software development) led to *new programming paradigms* and by extension, new programming languages
        * 1950s and early 1960s: simple applications
            - worry about machine efficiency
        * late 1960s: people efficiency became important
            - structured programming; top-down design and step-wise refinement
        * late 1970s: process-oriented to data-oriented
            - data abstraction
        * Middle 1980s: object-oriented programming
            - data abstraction + inheritance + polymorphism

- Language Categories or Paradigms
    + Language Categories
        * Imperative
            - Central features are variables, assignment statements, and iteration (i.e. things that *map directly to the hardware*)
            - include languages that support object-oriented programming 
            - include scripting languages
            - C, C++, C#, Go, Java, Javascript, Python, Rust etc.
        - Functional
            + Main means of making computations is by *applying functions to given parameters*
            + In its purest form defined as *mathematical functions*, there is no concept of memory, time or state **stateless**
            + LISP, Clojure, Erlang, Racket, Scheme, Haskell
            + [Functional Programming vs. Imperative Programming](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/functional-programming-vs-imperative-programming) 
        + Logic
            * Program statements describe *facts* (e.g. "Dong is Tom's father") and *rules* ("If x is y's father, and y is z's father, then x is z's grandfather")
            * The computer's job is to construct a pf based on the given axioms (facts + rules)
            * eg Prolog
        * MarkUp/Programming Hybrid
            - makeup languages extended to support some programming 
            - eg HTML and JSTL, XML and XSLT


- Language Design Trade-Offs and Examples
    - Reliability vs cost of execution
        + Java demands all references to array elements be checked for proper indexing, which leads to increased execution costs
    + Writability (flexibility) vs. reliability
        - C and C++ pointers are powerful and very flexible, but are the source of many programming errors
    - Readability vs. writability
        + APL provides many powerful operations (and a large amount of new symbols), allowing complex computations to be written in a compact program but at the cost of poor readability

- Implementation Methods (1.7)
    - **Compilation** - Programs are *translated* into machine language
        * Translate source program (in high-level language) into machine code
        * slow translation, fast execution (Compared to interpretation)
        * Translation occurs only once, but program is executed many times
        + Compilation process has phases
            * **Lexical Analysis** Converts characters in the source program into lexical units
            * **Syntax Analysis** Transforms lexical units into parse tree and check syntactic correctness
            * **Semantic Analysis** Type Checking and intermediate code generation
            * **Code generation** machine code is generated
            * (also include other steps such as optimization when generating intermediate code)
    - **Pure Interpretation** - Programs are interpreted/executed in a stepwise fashion by another program known as *interpreter*
        - No translation into machine language
            + Code is executed "on the fly"
        + rare for traditional high-level languages
        + advantages
            * execution is immediate
            * easier implementation of debugging programs
                - But errors are not found until actually executed
        + disadvantages
            * slower execution
                - 10 to 100 times slower than compiled programs
            - often requires more space
    - **Hybrid Implementation Systems** - A compromise between *compilers* and *pure interpreters*
        + A high-level language program is translated to an intermediate language
            * Translation can catch many errors early
            + Interpreting much simpler **byte code**
        + Faster than pure interpretation
            * (Byte code is really similar to machine code, but it is machine agnostic)
        + Initial implementations of Java: the *byte code* intermediate form provides portability to machines having a byte code interpreter and a run-time system (i.e., Java Virtual Machine)
    * Just-in-time implementation (wait them until they need to)
        - The intermediate language (bytecode) of methods are translated into machine code when they become *hot* (i.e. executed more often than some *threshold*)
        - Machine code version is kept for *subsequent calls*
        - E.g. source code (.java) -> Java compiler -> bytecode (.class) -> JVM -> Native Machine Code
        - eg Java, MATLAB, Python, .Net languages



### Chapter 2: Evolution of Major Programming Languages
- The history of languages that have had a *major impact* on the design and implementation of C++, Java, and Python, today's leading choices for programming languages
- Fortran 0 (1954) (section 2.3)
    + 1st successful high-level language 
    + Computers had *small memories* and were *unreliable*
    + Applications were *scientific*
    + *No* programming methodology or tools (from scratch)
    + *Machine Efficiency* was the most important concern
        * (focus is not on the programmer)
    * **was not implemented**
- Fortran I 
    + first *implemented version* of Fortran
    + Compiler released in April 1957 after 18 worker-years of effort
    + Outcome 
        - Programs larger than 400 lines rarely compiled correctly, mainly due to poor reliability of IBM 704
        - Code was very fast
        - Quickly became *widely used*
    + Features
        * Post-test counting loop (do-loop)
        * Three-way selection statement
            - Arithmetic IF - *if* (*expression*) *negative*, *zero*, *positive*
        - User-defined subprogram (No separate compilation)
        - Names could have up to six chars
        - No data typing statements (Types depended upon first letter of variable name)
    - John W. Backus 1977 Turing Award recipient
- Fortran II, IV, and 77
    + (earlier punch card represents a statement)
    + Fortran II
        * distributed in 1958 - independent compilation
    * Fortran IV 
        - explicit type declarations
        - legal if-statement *IF (ID. EQ. -9999) GO TO 76*
        - Subprogram names could be parameters
        - ANSI standard in 1966 (Fortran 66)
    - Fortran 77
        + New standard in 1978 
        + char string handling
        + logical loop control statement 
        + *if-then-else* statement 
    + Example: Subprogram Names Passed as Parameters
- Fortran 90
    + Changes from Fortran 77
        * Free formatting of code
        * Modules
        * Dynamic arrays
        * Array subsection references
        * Case-statement (switch)
        * Pointers
        * **Recursion** (no recursion makes program more efficient)
        * Parameter type checking
* Fortran Evaluation
    - Highly optimizing compilers (all versions before 90)
        + Type and storage of all variables are fixed before run time
    + Dramatically changed forever the way computers are used
    + (Alan Jay Perlis 1966 1st Turing Award recipient)

- ALGOL 60 - 1st step towards sophistication - section 2.5
    + (dead)
    + Environment of development
        * FORTRAN had (barely) arrived for IBM 70x
        * Many other languages were being developed, all for specific machines
        * No portable language; all were machine-dependent
            - ALGOL was the result of efforts to design a **universal** language
        - (lots of computer manufacturers in 1950s, both in US and Europe)
- ALGOL 58
    + Early design process
        * **joint European-American** committee met in 1958 (ACM and GAMM)
        * Goals of the language 
            - close to mathematical notation
            - good for describing algorithm
            - must be mechanically translate to machine code
    - Not intended to be implemented
        + Preliminary document for **international discussion**
        + But variations of ALGOL 58 were (MAD, JOVIAL)
            * (JOVIAL is the only kind of success one)
        + IBM initially enthusiastic, but dropped support by mid-1959
            * Adopting new programming languages was considered expensive
            * difficult to persuading programmers to try something new
            * difficult to get a "useful" first-generation compiler
        * (JOVIAL is adapted by military, translated JOVIAL to C, transpiler)
    - Language Design Features
        - Concept of data type was formalized
        - Compound statements (begin ... end)
        - *if-statement* had an *else-if* clause
        - Parameters were separated by mode (passed *in* and passed *out*)
            + `procedure(in1, in2, in3) =: (out1, out2, out3)` - does not return a value
+ ALGOL 60
    * Modified ALGOL 58 at a 6-day meeting in Paris in 1960
    * **Peter Naur** (2005 Turing Award recipient) was the main author or the paper describing ALGOL 60
    * Features:
        - Subprogram recursion
            - New for imperative languages
        - Two parameter passing methods
            + Pass-by-value and pass-by-name
        - Block structure (lexical scope)
            - Nested functions
        - Stack storage allocation 
+ ALGOL 60 Evaluation
    - Successes
        + **all subsequent imperative languages are based on it**
        + 1st machine-independent language
        + 1st language whose **syntax was formally defined** (using **BNF**)
        + it was the **standard way to publish algorithms** for over 20 years
        + **Robert Tarjan** (1986 Turing Award recipient) 
            - an example with Robert Tarjab's paper with the ALGOL 60 describing the algorithm
    * "showed 1st ways in which automatic computing could and should and did become a topic of academic concern" - Dijsktra (1973)
    - Failures
        + Never widely used, especially in U.S.
        + Lack of I/O definition made programs non-portable
        + Too flexible - hard to implement
            * few compilers implemented the entire language (eg pass-by-name)
        * Entrenchment of Fortran / lack of support from IBM
        * Formal syntax description (BNF) seemed to complex
- PL/I - Section 2.8
    + **Everything for everybody** - the first large-scale attempt to design a language that could be used for a broad spectrum of application areas
    + Background
        * Computing Landscape in 1964 - IBMâ€˜s point of view
            - Scientific computing
            - Business computing
        - By 1963
            - Scientific users began to need more elaborate I/O
            - Business users began to need floating points and arrays
        - The obvious solution
            + Build a new computer to do both kinds of applications
            + Design a new language to do both kinds of applications
            + The *swiss army knife* of programing languages
    - Features
        + Unit-level concurrency
        + Exception handling
        - Recursion could be turned on/off (for faster execution)
        - Pointers were included as data type
        - Cross section of arrays
    - Success
        + Used in **both** business and scientific applications in the 1970s
        + **Instructional** Vehicle in college in subset from (PL/C, PL/CS)
    + Concerns
        * Many new features were poorly designed
        * too large and too complex
        - No reserved keywords 
            + valid syntax IF ELSE THEN ELSE = THEN; ELSE THEN = ELSE
+ **ALGOL 68** - section 2.11
    * Design is based on the concept of **orthogonality**
        - A few basic concepts, plus a few combining mechanisms
        - eg of orthogonal 3 * (a := c + 4) + 2
    - Source of several new ideas
        + Language itself never achieved widespread use
    + Features
        * User-defined data types
        - Dynamic arrays
- Pascal (1971) - section 2.12
    + Developed by **Nikaus Wirth** (1984 Turing Award) (member of ALGOL 68 committee)
    + Small and simple, widely used for teaching (mid-1970s to mid 1990s)
        * an implementation of Pascal up and running on almost any platform in a week or so"
    * Pascal compiler toolkit (need an interpreter in Assembly)
        - Program (in Pascal) & Compiler (in P-Code) -- P-Code Interpreter -- program (in P-code)
            + rerun the generated program on the interpreter
        - similar to Java
    - Features
        + Enumeration types
        + Subrange types - create a type based on subrange of integer
- C (section 2.12)
    + Designed for systems programming (at **Bell Labs** by **Dennis Ritchie**)
        * **Dennis Ritchie** 1983 Turing for developing generic operating systems theory and specifically for the implementation of the UNIX os
    + Evolved primarily from BCPL, B but also ALGOL 68
    + Rich set of operations, but poor type checking
        * Very good for writing systems-level software (OS, etc.)
    + Initially spread through UNIX, which included a C compiler
        * Bootstrapping
            - Write the C compiler in Assembly language 
            - then write a C compiler in C and run through the Assembly language Compiler, and then use the C written one
    - Many ares of application
    - Ken Thompson (also developer of C) also developed Go
- Smalltalk - object-oriented programming(section 2.15)
    + developed at Xerox PARC in 1970's, by **Alan Kay**(Turing Award 2003), then Adele Goldberg
    + First full implementation of an **object-oriented programming**
    + also pioneered the **graphical user interface design** 
    + promoted Object-Oriented Programming (OOP)
- C++ - Combining Imperative and OO Programming (section 2.16)
    + Developed at Bell Labs by **Bjarne Stroustrup** in 1980
    + Evolved from C and **SIMULA 67**
        * Facilities for OOP taken partially from SIMULA 67
    * A large and complex language, in part because it supports both procedural and OOP
    * Rapidly grew in popularity along with OOP
* Java - Imperative-Based OO Language Section 2.17
    - Developed at Sun (became oracle later) in the early 1990s
        + C and C++ were not satisfactory for **embedded electronic devices**
    + Based on C++
        * Significantly **simplified** (does not include struct, union, pointer arithmetic, and half of the assignment *coercions* of C++)
        * supports **only OOP**
        * has references, but not pointers
        * Garbage collected memory management
    * Evaluation
        - Eliminated many unsafe features of C++
        - **Portable**: Java Virtual Machine concept, JIT compilers
        - Use increased faster than any previous language
            + Mostly due to academic **acceptance** and **support** from certain corporations (*anti-Microsoft* by IBM, Sun, and Netspace communications)

### Chapter 3 Describe Syntax
- section 3.1 - 3.3 
- intro
    + **Syntax** - the form of structure of the expressions, statements, and program units
    + **Semantics** - the meaning of the expressions, statements, and program units
    + Syntax and semantics provide a language's definition
    + Language Definition Users
        * Initial evaluators (the language designers)
        * Implementers (the compiler writers)
        * Programmers (the users of the language)
* Review Compilation
    - Lexical Analysis - converts characters in the source program into lexical units
    - Syntax Analysis - transforms lexical units into parse tree and check syntactic correctness
- Lexical Analysis
    - source code -> long string of ASCII characters
    - The lexical analyzer splits it into ***tokens***
    - TOKEN
        + a syntactic category that forms a class of ***lexemes*** 
        + identifiers, literals, keywords, operators, punctuation
    + LEXEME
        * a sequence of characters that matches the pattern for a token
            - myVariable 123 5.67 true char public + - * / l, {}
            - *instances* of tokens/classes
    - Whitespace (exception: Python) and comments discarded
    - Detecting Code Clones After Lexical Analysis 
        + spread the original source code through lexical analysis to lexmmes
        + ignoring lexeme values
        + identifying duplicate sequences 
        + Detected clones - back to the original code block associated with the lexmmes
        + ccfiner.net use this approach to detect clones
+ Describing syntax
    * (syntax can be right, though semantically might not be)
    * Meta-language
        - a language used to define other languages
    - Grammar 
        + a meta-language used to defined the *syntax* of a language
- Formal Definition of Languages
    + *recognizers* 
        * a *recognition device* reads input strings over the alphabet of the language and decided whether the input strings to the language
        * produces a *yes/no* answer as to whether the input string in a valid sentence in th language
            - or whether the given file contains a valid program for the programming language
        - example: syntax analysis part of a compiler
    + *Generators*
        - a device that generates valid sentences of a language 
        - generates a random sentence each time
        - thus limited use as a language descriptor
        - one can determine the syntax of a particular sentence is syntactically correct by *comparing* it to the *structure of the generator*
            + compilers are often created by *specifying a generator description* that is then *used to create a recognizer*
            + much of this forms the basis of computational theory 
    + Language generator -> language -> language recognizer
- BNF and context-free grammars (developed independently)
    + developed Noam Chomsky (a linguist)
    + four classes of *generative devices* of *grammars* that define four classes of languages
    + two of them (*context-free* and *regular*) were later useful for describing the syntax of programming languages
        * regular - tokens, context-free - syntax analyzer
- Backus-Naur Form (1959)
    + Invented by John Backus to describe Algol 58
        * introduced Peter Naur's contributions while describing Algol 60
    + A natural notation for describing syntax
    + BNF is equivalent to context-free grammars
- A BNF grammar consist of 
    + start symbol, fine set of production rules, finite set of terminal symbols, fine set of nonterminal symbols
    + example of BNF production Rules (going from left to right)
    ```
    <while_loop> -> while (<logic_expr>) <stmt>
    <if_stmt> -> if (<logic_expr) then <stmt>
    ```
    + *Terminals* are Lexemes or tokens (eg while(), if() then)
    + *Nonterminal symbols* are abstractions used to represent classes of syntactic structures (`<while_loop>`, `<logic_expr>`, `<stmt>`) (consists of terminals in the leaves)
    + a *production rules* has 
        * a LHS, which is a single *nonterminal*, and
        * a RHS, which is a string of *terminals* and/or *nonterminals*
    * A *derivation* is the **repeated application of rules**, starting with the *start symbol* (a special nonterminal) and ending with a *sentence* (a string of only terminal symbols)
        - start symbol - (derivation) -> sentence (a string of only terminal symbols)

- BNF Rules
    + An abstraction (or nonterminal symbol) can have more than one RHS (eg using `|` to separate)
- Describing Lists
    + Syntactic lists are described using *recursion*
    ```
    <stmt_list> -> <stmt> 
        | <stmt>; <stmt_list> // lists
    ```
- Derivations
    + **Sentential form** - Every string of (non terminal or terminal) symbols in a derivation 
        * The bottom one is also called **sentence** e.g. a = c + 2
- **Leftmost Derivation**
    + The *leftmost nonterminal* in each sentential form is the one that is expanded next
    - (opposite - rightmost Derivation)
- **Parse Tree**
    + A hierarchical representation of a derivation
- Ambiguity in Grammars
    + A grammar is *ambiguous* if and only if it generates a sentential form that has two more meanings
    + e.g. operator precedence 
- An Unambiguous Expression Grammar
    + use the parse tree to indicate precedence levels of the operators
    ```
    <expr> -> <expr> - <term> | <term>
    <term> -> <term> / const | const
    ```
- Associativity of Operators
    + Operator associativity can also be indicated by a grammar
        * eg `<expr> -> <expr> + <expr> | const`
    * unambiguous `<expr> -> <expr> + const | const`
    * parenthesis ? `expr` -> `term` -> `factor` -> (`expr`) (finish up at very bottom)
* when we do the expansion: One expansion at a time (leftmost mostly)


